{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 12: Hidden Markov Models\n",
    "\n",
    "CBIO (CSCI) 4835/6835: Introduction to Computational Biology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overview and Objectives\n",
    "\n",
    "Today, we'll cover our first true computational modeling algorithm: Hidden Markov Models (HMMs). These are very sophisticated techniques for learning and predicting information that comes to you in some kind of sequence, whether it's an amino acid primary structure, a time-lapse video of molecules being trafficked through cells, or the construction schedule of certain buildings on the UGA campus. By the end of this lecture, you should be able to:\n",
    "\n",
    " - Define HMMs and why they can be useful in biological sequence alignment\n",
    " - Describe the assumptions HMMs rely on, and the parameters that have to be learned for an HMM to function\n",
    " - Recall the core algorithms associated with training an HMM\n",
    " - Explain the weaknesses of HMMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 1: `CG`-islands and Casinos\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    " - Given four possible nucleotides (A, T, C, and G), how probable is one of them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - How probable is a **dinucleotide pair**, aka any two nucleotides appearing one right after the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - As we discussed in last week's lecture, these raw probabilities aren't reflected in the real world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - In particular, `CG` is typically underrepresented, clocking in at a frequency considerably less than the \"expected\" $\\frac{1}{16}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `CG-islands`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CG` is the least frequent dinucleotide (why?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " - The `C` is easily methylated, after which it has a tendency to mutate into a `T`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - However, methylation is suppressed around genes in a genome, so `CG` will appear at relatively *high* frequencies within these `CG`-islands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Finding `CG`-islands, therefore, is an important biological problem!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Fair Bet Casino"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say you've wandered into a casino, hoping to make enough money to keep you from ever having to write another sequence alignment function using dynamic programming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the **Fair Bet Casino**: the game is to flip two coins: a fair coin (F) and a biased coin (B)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " - Only one of the coins is ever flipped at a time, the outcome of which is always either Heads (H) or Tails (T)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " - The fair coin F will give Heads or Tails with equal probability (0.5), so $P(H | F) = P(T | F) = \\frac{1}{2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " - The biased coin B will give Heads with probability 0.75, so $P(H|B) = \\frac{3}{4}$ and $P(T|B) = \\frac{1}{4}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " - The dealer is a sly crank: he'll change between F and B with probability 0.1, so **you never actually know which coin he's using.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**How can we tell whether the dealer is using F or B?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The \"Decoding Problem\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Given**: A sequence of coin flips, such as `HHHHHHHHHH` or `HTHTHTHTHT`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to be able to \"grade\" the sequence of coin flips and assign the \"most likely\" corresponding sequence of Fair or Biased coins making each flip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are, in effect, *decoding* the observed sequence (Heads or Tails) to recover the *underlying state* (Fair or Biased)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Simple <s>dog</s> Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with an easy case: the dealer never switches coins. Our problem can then be formalized as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Representing the sequence of coin flips as $x$, $P(x | F)$ is the probability of generating the sequence *given* the dealer is using the Fair coin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Similarly, $P(x | B)$ is the probability of generating the sequence $x$ *given* the dealer is using the Biased coin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can compute this directly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we have\n",
    " - A sequence of length $n$\n",
    " - $k$ of the elements are Heads\n",
    " - $n - k$ are Tails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$P(x | F) = \\prod_{i = 1}^n \\frac{1}{2} = \\left(\\frac{1}{2}\\right)^k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what would the probability using a Biased coin be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$P(x | B) = \\prod_{i = 1}^n \\left(\\frac{3}{4}\\right)^k \\left(\\frac{1}{2}\\right)^{n - k} = \\frac{3^k}{4^n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So if we made a couple of examples..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Example 1:** `HTHTHT` has a length of 6, with 3 Heads and 3 Tails.\n",
    " - $P(x | F) = \\left(\\frac{1}{2}\\right)^6$, or 0.15625 (roughly 15.5%)\n",
    " - $P(x | B) = \\frac{3^3}{4^6}$, or 0.006592 (roughly 0.65%)\n",
    " \n",
    "Which is more likely?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Example 2:** `HHHHHH` has a length of 6, with 6 Heads and 0 Tails.\n",
    " - $P(x | F) = \\left(\\frac{1}{2}\\right)^6$, or 0.15625 (roughly 15.5%)\n",
    " - $P(x | B) = \\frac{3^6}{4^6}$, or 0.1779785 (roughly 17.8%)\n",
    " \n",
    "Which is more likely?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Pause: any questions?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Part 2: The Anatomy of HMMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HMMs can be viewed as \"machines\" with $k$ *hidden states*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Depending on what state the HMM is in, it can \"emit\" different outputs from an alphabet of all possible outputs--we'll call this alphabet $\\Sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Each hidden state has its own probabilities for what kind of outputs it likes to emit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - The HMM can also switch between hidden states with some probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "While in a certain state, the HMM asks itself two questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1. What state should I move to next? (it could \"move\" to the same state it's currently in)\n",
    " 2. What symbol from the alphabet $\\Sigma$ should I emit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hidden States"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *hidden* states (which give HMMs their name, **Hidden** Markov Models) are the key component and the part of HMMs that make building them tricky."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't know whether the dealer is using a Fair or Biased coin; we don't know whether this portion of an amino acid sequence is an $\\alpha$-helix or a $\\beta$-sheet; we don't know whether this portion of the genome is a `CG` island."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The goal of decoding with HMMs is to make *the best possible guess* as to the sequence of hidden states, *given* the sequence of emitted symbols."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### HMM Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its parameters are to an HMM what our genomes are to us: they're essentially the genetic makeup that distinguishes one HMM from another. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " - $\\Sigma$: We've already seen this: it's the set of $m$ possible emission symbols (for an amino acid sequence, this is the amino acid symbols; for a genome, this is the nucleotides; for the Fair Bet Casino, this is Heads or Tails)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " - $Q$: This is the set of $k$ hidden states (think of it as a list)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " - $A$: This is a $k \\times k$ matrix containing probabilities of switching between hidden states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " - $E$: This is a $k \\times m$ matrix containing probabilities of emitting a certain symbol from $\\Sigma$ *given* that the HMM is in a certain hidden state from $Q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `A` matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This encodes the probability of switching between hidden states. Using our Fair Bet Casino example, there are only two hidden states, so the matrix will be $2 \\times 2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![A_matrix](HiddenMarkovModels/A_matrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "A = \\begin{pmatrix}\n",
    "0.9 & 0.1 \\\\\n",
    "0.1 & 0.9\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `E` matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matrix encodes the probabilities of emitting certain symbols, given that the HMM is in a certain hidden state. Using our Fair Bet Casino example, with two hidden states and two possible output symbols, this matrix is also $2 \\times 2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![E_matrix](HiddenMarkovModels/E_matrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "E = \\begin{pmatrix}\n",
    "0.5 & 0.5 \\\\\n",
    "0.25 & 0.75\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Taken together, the parameters of our Fair Bet Casino HMM would look something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fbc_params](HiddenMarkovModels/fbc_params.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### State Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another perhaps more intuitive perspective would be to view an HMM as a \"state machine\": each possible state of the HMM is represented as a node in a graph, with arrows connecting the nodes that are weighted based on their probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![state_machine](HiddenMarkovModels/state_machine.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hidden Paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observable sequence of emitted symbols (coin flips, amino acids, nucleotides, etc) is one sequence. However, the corresponding sequence of hidden states is known as the *hidden paths*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is usually represented as $\\vec{\\pi} = \\pi_1, ..., \\pi_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - (Note: both the hidden path and the observed sequence have the same length, $n$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Consider the sequence $x =$ `01011101001` and the hidden path $\\vec{\\pi} =$ `FFFBBBBBFFF`. This is how we would list out the probabilities:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![hidden_paths](HiddenMarkovModels/hidden_paths.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 3: The Decoding Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the question you'll most often ask of HMMs: given a set of observed sequences, find the **optimal hidden path** that would have generated the observed sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formally, this is defined as the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Input: A sequence of observations $\\vec{x} = x_1, ..., x_n$ generated by an HMM $M(\\Sigma, Q, A, E)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Output: A hidden path $\\vec{\\pi} = \\pi_1, ..., \\pi_n$ that maximizes $P(x | \\pi)$ over all possible hidden paths $\\vec{\\pi}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Any ideas?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Who was Andrew Viterbi?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andrew Viterbi used the the *Manhattan edit graph model* to solve the decoding problem! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Yep, dynamic programming is back!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the same matrix abstraction as before, except instead we only have one sequence of length $n$, and then the other axis is used for the $k$ hidden states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It looks something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![hmm_edit_graph](HiddenMarkovModels/hmm_edit_graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So, instead of having one of 3 decisions at each vertex--insertion, deletion, or match/mutation--we have one of $k$ decisions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![align_vs_decode](HiddenMarkovModels/align_vs_decode.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Properties of the Viterbi algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each possible path through the graph has probability $P(x | \\pi)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - You're picking a specific hidden path through the graph, so you've fixed $\\pi$, and you already know the observed sequence $x$, so that makes the probability straightforward to compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The Viterbi algorithm finds the path that gives the *maximal possible* $P(x | \\pi)$, over all possible paths through the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Consequently, this algorithm has $\\mathcal{O}(nk^2)$ runtime complexity (why?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 4: The Forward-Backward Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the question you'll ask of HMMs second in frequency only to the decoding problem: given a set of observed sequences, compute the probability the system was in a certain state at a certain time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formally, this is defined as the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Input: A sequence of observations $\\vec{x} = x_1, ..., x_n$ generated by an HMM $M(\\Sigma, Q, A, E)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Output: The probability the HMM was in state $k$ when it emitted symbol $x_i$, or $P(\\pi_i = k | x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Any ideas?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Who was Forward-Backward?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dunno, but this question is answered using a combination of *two* algorithms: the **forward algorithm**, and the **backward algorithm**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fb_pic](HiddenMarkovModels/fb_pic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Forward Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's define the *forward probability* as $f_{k, i}$: the probability of emitting a *prefix* $x_1, ..., x_i$ and reaching state $\\pi = k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put another way, this means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " - we've already observed (read: we're *given*) a subsequence of $i$ out of the $n$ symbols: $x_1$ through $x_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - we're asking the probability of having reached state $k$ (whatever that is)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[Hopefully] Easy way to remember: we've fixed the first $i$ symbols, but we have to move forward to see the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Backward Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define the *backward probability* as $b_{k, i}$: the probability of being in state $\\pi_i = k$, and emitting the *suffix* $x_{i + 1}, ..., x_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put another way, we're now looking at"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " - a system where we now assume (read: we're *given*) that we've observed the other subsequence of symbols, $x_{i + 1}$ through the end $x_n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - asking for the probability that starting in state $k$ (whatever that is) would have then generated the aforementioned suffix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[Hopefully] Easy way to remember: we've fixed the last $n - i$ symbols, but have to move backward to get the probability of state $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Backward-Forward Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asking at any given coin flip whether the casino dealer is using a fair or biased coin is, in fact, a function of both the forward and the backward probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Formally:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![forward_backward](HiddenMarkovModels/forward_backward.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**(does this look familiar???)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 5: Why HMMs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![why](HiddenMarkovModels/Why.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Profile HMMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we're interested in finding a distant cousin of functionally related protein sequences in a family."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This family has diverged so much that pairwise (e.g. Hamming) comparisons are weak, and may therefore fail a statistical significance test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, these weak similarities may show up across *many members of the family*, indicating a correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The goal, then, is to align *all* members of the family at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A family of related proteins can be represented by their multiple alignment and the corresponding **profile** (hence, the name)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Aligned DNA sequences can be represented as a $4 \\times n$ profile matrix, reflecting the frequencies of nucleotides in every aligned position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![aligned_nucleotides](HiddenMarkovModels/aligned_nucleotides.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Similarly, a protein family can be represented using a $20 \\times n$ profile representing amino acid frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Multiple alignment of a protein family can show variations in conservation along the length of the protein:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - For example, after aligning many globin proteins, biologists recognized that the helices region in globins are far more conserved than others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A **profile HMM**, then, is a probabilistic representation of a multiple alignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model can then be used to find and score less obvious potential matches of new protein sequences to find distant family members."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Consists of three states (any guesses?):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " - Match states $M_1, ..., M_n$\n",
    " - Insertion states $I_0, ..., I_n$\n",
    " - Deletion states $D_1, ..., D_n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![profile_hmm](HiddenMarkovModels/profile_hmm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Comparison to dynamic programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our walk through the profile HMM is pretty much identical to a walk along the Manhattan edit graph:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![manhattan_edit](HiddenMarkovModels/manhattan_edit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How to build a Profile HMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In four easy steps!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**1:** Use BLAST to separate a protein database into families of related proteins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2:** Construct a multiple alignment for each protein family."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3:** Construct a profile HMM and optimize the parameters of the model (transition and emission probabilities)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4:** Align the target sequence (the one you're not sure about) against each profile HMM to find the best fit between the target sequence and a particular HMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**What problem is #4 solving?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Modelilng globin proteins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Globins represent a large collection of protein sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "400 globin sequences were randomly selected from all globins and used to construct a multiple alignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple alignment was used to train a profile HMM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Another 625 globin proteins (*not* used to train the profile HMM) were selected, along with a few hundred other non-globin proteins chosen randomly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guess which group--the globins or non-globins--scored better against the profile HMM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Great way to separate and identify families of proteins; in this case, separate (automatically!) globins and non-globins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Administrivia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - How is Assignment 3 going?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - If you're having trouble with the assignments, **please come to office hours!** (Wednesdays, 12:30 - 2:30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - If you're concerned about your performance on the first two assignments, please get in touch with me. **Still plenty of time left to ace the course!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - The midterm \"exam\" (in 1.5 weeks!) will be located in the **Miller Learning Center, Room 368.** It's a computer lab! So don't worry about bringing your own computer (though you can if you want). It'll be a **group exercise** answering a specific question, and the work will be done on JupyterHub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Additional Resources\n",
    "\n",
    " 1. Matthes, Eric. *Python Crash Course*, Chapter 10. 2016. ISBN-13: 978-1593276034\n",
    " 2. Model, Mitchell. *Bioinformatics Programming Using Python*, Chapter 2. 2010. ISBN-13: 978-0596154509"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
